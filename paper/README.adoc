= homecloud : a cloud at home with Docker Swarm, Ceph and Syncthing
// METADATA
:doctype: article
:author: Thibault Morin
:revdate: 2020-11-28
:homepage: https://github.com/tmorin/homecloud-ansible
:toc:
// FOOTNOTES
:fn-p64_disclamer: footnote:[The author, Thibault Morin, declares that there is no conflict of interest with PINE64. Thibault Morin is just a regular consumer of PINE64 products.]
:fn-dmz_skipped: footnote:[To reduce the complexity of the diagram, the demilitarized zone of the home network is skipped.]

== Introduction

`homecloud` aims to provide a _cloud like_ <<cld>> at home, primarily hosted on ARM boards like Raspberry Pi and based on Docker Swarm, Ceph and Syncthing.

The main artifact is an Ansible <<ans>> collection designed to bootstrap a ready to use _cloud like_ environment.

== The original use case

The intentions which led to the `homecloud` creation match ones related to the Framasoft initiative: De-google-ify Internet <<dgo>>; and its following one: Contributopia <<cpa>>.

Therefore, the original use case is:

- be able to host all services which care about private data: contacts, calendars, mobile pictures, private chats, personal projects, ...
- ba able to manage a human size of users, basically a family.

Because big names of cloud computing are de facto discarded, the most affordable solution is to host all those services in a cluster of low cost ARM boards, especially at home.

Nevertheless, many other use cases could match the need of a self-hosted cluster, of low cost ARM boards, which provides the principal characteristics of cloud computing.

== The cluster

The `homecloud` cluster is a network of computers, especially servers, which work together in order to provide services available from Internet, to end users.

Despite many providers offer ARM boards all around the world, this article focus exclusively{fn-p64_disclamer} on some boards manufactured by PINE64 <<p64>>.

The next parts of this article may refer to the following cluster layout:

|===
|Hostname|Type|Architecture|Memory|Operating System

|hc1-n1
|ROCK64 <<r64>>
|aarch64
|4G
|Armbian

|hc1-n2
|ROCK64 <<r64>>
|aarch64
|4G
|Armbian

|hc1-n3
|ROCK64 <<r64>>
|aarch64
|4G
|Armbian

|hc1-n4
|PINE A64-LTS <<plts>>
|aarch64
|2G
|Armbian
|===

.The hc1 cluster layout
image::cluster_layout.png[The hc1 cluster layout]

The four boards are inter-connected using the home's router which is already connected to internet{fn-dmz_skipped}.

== The user services

The primarily purpose of the cluster is to host the following user services:

- Nextcloud: a suite of client-server software for creating and using file hosting services <<ncd>>
- calibre-web: a webapp for browsing, reading and downloading eBooks stored in a Calibre database <<caw>>
- Syncthing: a free, open-source peer-to-peer file synchronization application <<syt>>
- Samba: the standard Windows interoperability suite of programs for Linux and Unix <<smb>>

.The provided user services of the hc1 cluster
image::user_services_layout.png[The provided user services of the hc1 cluster]

However, to properly operate the above listed services, two hosting strategies are highly emphasis: containerized workloads and container orchestration.

== The hosting strategies

The first strategy, the containerized workload (i.e. the famous _containers_), provides many benefits about the packaging, distribution and usage of the services them-self <<rhc>>.

The key characteristics are:

- Portability : a container can be easily deployed in a container environment whatever the host's operating system within the respect of the container's and host's architecture.
- Configurability : a container can be easily configured about its infrastructure (cpu, memory ...) but also about the underlying workload (overriding containerized file or environment variables).
- Isolation : a container cannot exceed its infrastructure limit and so cannot impact sibling running containers.
- Efficient disk usage : a containerized workload needs less disk usage than virtualized one.

The second one, the container orchestration, provides also many benefits about the overall handling of containerized workloads <<rhco>>.

The key characteristics are:

- Automatic deployments : a container orchestrator manages it-self the deployment process of containerized workloads across the nodes.
- Container management : a container orchestrator provides services to monitor and interact with containerized workloads deployed among the cluster nodes.
- Resource allocation : a container orchestrator monitors and manages the resources to satisfy the requirements of the deployed containerized workloads.
- Networking configuration : a container orchestrator manages it-self the networking configuration to provide isolation and/or inter-connection between containerized workloads according to their needs.

However, a wish list of services, and a couple of hosting strategies are not enough to provide an efficient cluster.
Some pieces are still missing: a set of building blocks able to support the services embracing the hosting strategies.

== The building blocks

The purpose of the building blocks is to support the execution of the user services.
Some building blocks are parts of the virtual world whereas others to the physical one.

.User Services and Building Blocks
image::user_services_and_building_blocks.png[User Services and Building Blocks]

=== The container engine and orchestrator

Docker is one of the most popular technology about _container_, and moreover, ready-to-use to almost all architectures <<dok>>.
Therefore, because `homecloud` must be easy to bootstrap and configure for at least `amd64` and `aarch64` architectures, Docker is a good candidate for the container engine.

Luckily Docker shipped a ready-to-use container orchestrator: Docker Swarm <<dsw>>.applications will be hi
Therefore, according to the _domestic_ usage of `homecloud`, Docker Swarm is a good candidate for the container orchestration especially because no overhead will be added to the technical stack.

Nevertheless, additional building blocks have to be added to the virtual stack.

=== The cluster availability

Basically when a request comes from Internet, the router has to redirect it to the cluster using the port forwarding technique <<pft>>.
Therefore, the router must be configured with an IP able to handle the forwarded requests.

In the `homecloud` context, the configured IP is one of anyone of the cluster nodes, because Docker Swarm is internally able to forward requests to the right node whatever the entry point <<dnt>>.

However, IP addresses can be dynamics and moreover the node availability cannot be guaranty.
It means the configured IP could become unallocated in the future in case of dynamic IP, or pointing to a node which stops to work properly.
Therefore, the cluster availability cannot be guaranty.

One of the simplest solutions to prevent unavailability of the cluster is to use the virtual server technique <<vswt>>.
That means, from the router point of view, the cluster is just a unique server which can be reached with a unique IP address which will never ever change.

Keepalived is one of the most popular implementations of the virtual server technique <<kad>>.
Moreover, it can be easily containerized and configured.
Therefore, Keepalived is a good candidate for the virtual server technique.

An overview of the Keepalived integration is available in the appendix: <<appendix_keepalived_integration>>.

Now the cluster is _highly available_, the next topic is to be sure the containerized workloads are _highly available_ too.

=== The distributed file system

Deploying containers and providing their high availabilities on a cluster is easy with Docker Swarm.
However, it doesn't manage the availability of the containers' data among the nodes.

For instance, if a container hosting a database is destroyed and then re-created on a new node by the orchestrator, by default, the new container won't start with the data related to the destroyed one.

In order to get the availability of the data among the nodes of the cluster, a distributed file system has to be configured.

Ceph is one of the most popular technology about distributed file system <<cep>>.
Moreover, it can be easily integrated in a Docker environment <<dvc>>.
Therefore, Ceph is a good candidate for the distributed file system.

Now containers are able to recover their data over their lifecycles, there is another topic to deal with: how final services will be found and reached from Internet?

=== The reverse proxy

The purpose of a reverse proxy is to handle the requests coming from the external world.
In the `homecloud` context, the reverse proxy handles the requests coming from Internet.
The handling of incoming requests can be straight forward or much complex: enhancement of requests, security, load balancing ...
At the end, the reverse proxy routes the requests to the right _back end_ located in the internal world.

Traefik is one of the most popular technology about reverse proxy <<tra>>.
Moreover, it can be easily integrated in a Docker environment.
Therefore, Traefik is a good candidate for the reverse proxy.

Well, the cluster is strong enough to serve the user services, nevertheless unexpected events can occur and lead to unavailability of the cluster.
Unavailability are not welcome and another building block should prevent them, monitoring the cluster's status and broadcasting alerts.

=== Monitoring and alerting

The Influxdata stack is one of the most popular technology about monitoring and alerting <<inf>>.
Especially because the Influxdata stack provide all expected components:

- Telegraf: an agent to collect metrics <<ite>>
- InfluxDB: a database to store metrics <<idb>>
- Chronograf: a front-end to render the metrics <<ich>>
- Kapacitor: a data processing engine to detect anomalies and send alerts <<ika>>

Therefore, Influxdata is a good candidate for the monitoring and alerting stack.

However, once the monitoring & alerting stack detects an anomaly and emits an alert then usually actions have to be executed.
Therefore, a final build block has to be defined: the management of the Docker Swarm cluster.

=== Docker Swarm management

The management of a Docker Swarm cluster can be done using the command line interface provided by default.
However, its usage requires access to the terminals of cluster's master nodes.
Another way is to use a web-app connected to a backend which will be able to directly deals with Docker Swarm daemon.
So that, the management of the Docker Swarm can be done without access to the cluster's master nodes.

Portainer is one of the most popular solutions to manage Docker Swarm clusters from a web-app <<por>>.
Moreover, it provides natively the support of Docker Swarm for the expected architectures.
Therefore, Portainer is a good candidate for the management of the Docker Swarm cluster.

At this point, all building blocks have been introduced, it's time to summarize the cluster's components.

== The components overview

All `homecloud` components, services and building blocks, can breakdown in three categories:

1. services of Operating System
2. orphan Docker containers
3. stacks of Docker Swarm

.The components hierarchy
image::components_hierarchy.png[]

[appendix]
[#appendix_keepalived_integration]
== Keepalived integration

For each board, i.e. node of the cluster, the Keepalived application runs in a Docker Container which is executed in the Docker Engine.
The execution requires the _privileged_ flag, and the capability _NET_ADMIN_.
Therefore, each Keepalived instance can manage the virtual network interface of the virtual IP.

.Overview of the Keepalived integration
image::building_blocks_keepalived.png[Overview of the Keepalived integration]

[bibliography]
== References

*Opinions*

- [[[dgo]]] De-google-ify Internet, https://degooglisons-internet.org/en
- [[[cpa]]] Contributopia, https://contributopia.org/en

*Concepts*

- [[[cld]]] The NIST Definition of Cloud Computing, https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf
- [[[rhco]]] What is container orchestration?, https://www.redhat.com/en/topics/containers/what-is-container-orchestration
- [[[rhc]]] What's a Linux container?, https://www.redhat.com/en/topics/containers/whats-a-linux-container
- [[[doha]]] What is High Availability?, https://www.digitalocean.com/community/tutorials/what-is-high-availability
- [[[pft]]] Port Forwarding, https://en.wikipedia.org/wiki/Port_forwarding
- [[[vswt]]] What is virtual server?, http://www.linux-vs.org/whatis.html

*Hardware*

- [[[p64]]] PINE64, https://www.pine64.org
- [[[plts]]] PINE A64-LTS, https://www.pine64.org/devices/single-board-computers/pine-a64-lts
- [[[r64]]] ROCK64, https://www.pine64.org/devices/single-board-computers/rock64

*Technologies*

- [[[ans]]] Ansible, https://www.ansible.com
- [[[caw]]] calibre-web https://github.com/janeczku/calibre-web
- [[[cep]]] Ceph, https://ceph.io
- [[[kad]]] Keepalived, https://www.keepalived.org
- [[[dok]]] Docker, https://www.docker.com
- [[[dsw]]] Docker Swarm, https://docs.docker.com/engine/swarm
- [[[dnt]]] Docker - Networking overview, https://docs.docker.com/network
- [[[dvc]]] docker-volume-cephfs https://gitlab.com/n0r1sk/docker-volume-cephfs
- [[[ich]]] Chronograf, https://www.influxdata.com/time-series-platform/chronograf
- [[[idb]]] InfluxDB, https://www.influxdata.com/time-series-platform/
- [[[ika]]] Kapacitor, https://www.influxdata.com/time-series-platform/kapacitor
- [[[inf]]] Influxdata, https://www.influxdata.com
- [[[ite]]] Telegraf, https://www.influxdata.com/time-series-platform/telegraf
- [[[ncd]]] Nextcloud, https://nextcloud.com
- [[[por]]] Portainer, https://www.portainer.io
- [[[smb]]] Samba, https://www.samba.org
- [[[syt]]] Syncthing, https://syncthing.net
- [[[tra]]] Traefik, https://traefik.io
